paperID,authors,organisations,title,title_plain,keywords,topics,abstract,abstract_plain,all_emails,acceptance
101,"Hoover, David Lowell","New York University - Main Campus, United States of America","Booth Tarkington, Blindness, Dictation, and the Durability of Style","Booth Tarkington, Blindness, Dictation, and the Durability of Style","style, dictation, literary composition","English, North America, 19th Century, 20th Century, attribution studies and stylometric analysis, Literary studies","<p>When Booth Tarkington suffered from severe vision problems and temporary complete blindness in 1929, he began to dictate almost all of his literary works, a practice he continued even after regaining good eyesight in 1931. This submission investigates the possible effects of this change in mode of composition on Tarkington's style. The tentative conclusion is that there is almost no evidence of any effect, which suggests that authorial style can be quite durable in the face of a complete change in the way the author produces his text.</p>
","When Booth Tarkington suffered from severe vision problems and temporary complete blindness in 1929, he began to dictate almost all of his literary works, a practice he continued even after regaining good eyesight in 1931. This submission investigates the possible effects of this change in mode of composition on Tarkington's style. The tentative conclusion is that there is almost no evidence of any effect, which suggests that authorial style can be quite durable in the face of a complete change in the way the author produces his text.",david.hoover@nyu.edu,Short Presentation
102,"Hoover, David Lowell","New York University - Main Campus, United States of America",Testing Rolling.Classify,Testing Rolling.Classify,"Rolling Classify, collaboration, simulation","Europe, English, North America, 19th Century, 20th Century, Contemporary, attribution studies and stylometric analysis, Humanities computing, Literary studies","<p>Rolling.Classify is a recently developed tool for studying collaboration (Eder, Rybicki, and Kestemont 2016; Eder 2016) that builds on earlier work that tested successive overlapping sections of texts (van Dalen-Oskam and van Zundert 2007, Burrows 2010, Hoover 2012).</p>
<p>The power and ease of use of Rolling.Classify (and its related Rolling.Delta) have led to several studies based on various kinds of texts.. Rigorous testing of this new method on problems with known solutions seems especially important because its results vary greatly with the choice of classification method other parameters. I will begin with simulated collaborations comprising text sections of varied lengths assembled to model different kinds of collaboration. I will then test collaborations with known contributions by the authors, and finally some in which no clear evidence of the nature of the collaboration exists.</p>
","Rolling.Classify is a recently developed tool for studying collaboration (Eder, Rybicki, and Kestemont 2016; Eder 2016) that builds on earlier work that tested successive overlapping sections of texts (van Dalen-Oskam and van Zundert 2007, Burrows 2010, Hoover 2012).
The power and ease of use of Rolling.Classify (and its related Rolling.Delta) have led to several studies based on various kinds of texts.. Rigorous testing of this new method on problems with known solutions seems especially important because its results vary greatly with the choice of classification method other parameters. I will begin with simulated collaborations comprising text sections of varied lengths assembled to model different kinds of collaboration. I will then test collaborations with known contributions by the authors, and finally some in which no clear evidence of the nature of the collaboration exists.",david.hoover@nyu.edu,Short Presentation
103,"Anderson, Talea","Washington State University, United States of America","Effect of Promotion, Rank, and Tenure Guidelines on Open Data Distribution","Effect of Promotion, Rank, and Tenure Guidelines on Open Data Distribution",open data,"English, North America, Contemporary, data publishing projects, systems, and methods, data, object, and artefact preservation, Library & information science","<p>Promotion, rank, and tenure (PRT) guidelines have been cited as a key motivation for why and how faculty choose to publish their research. Open-access (OA) advocates have noted in particular that lukewarm or negative portrayals of OA venues in PRT guidelines can result in decreased participation in OA publishing. In response to this concern, some universities have begun to experiment with adding language to PRT guidelines that invites broader participation in publishing venues, including OA publishing. Of interest in this study is the impact of PRT guidelines on the distribution of open data. Specifically, this poster will rely on information gathered from SHARE to consider the prevalence of participation in open data publishing at schools that include more expensive PRT guidelines.</p>
","Promotion, rank, and tenure (PRT) guidelines have been cited as a key motivation for why and how faculty choose to publish their research. Open-access (OA) advocates have noted in particular that lukewarm or negative portrayals of OA venues in PRT guidelines can result in decreased participation in OA publishing. In response to this concern, some universities have begun to experiment with adding language to PRT guidelines that invites broader participation in publishing venues, including OA publishing. Of interest in this study is the impact of PRT guidelines on the distribution of open data. Specifically, this poster will rely on information gathered from SHARE to consider the prevalence of participation in open data publishing at schools that include more expensive PRT guidelines.",talea.anderson@wsu.edu,Poster
104,"Palladino, Chiara ; Zhang, Anna ; Foradi, Maryam ; Yousef, Tariq "," Furman University, United States of America;  Digital Humanities, University of Leipzig, Germany;  NLP Group, University of Leipzig, Germany",How to Read All Languages: Translation Alignment with Ugarit,How to Read All Languages: Translation Alignment with Ugarit,"translation alignment, digital pedagogy, language learning","Comparative (2 or more geographical areas), English, BCE-4th Century, 5th-14th Century, 15th-17th Century, electronic literature production and analysis, public humanities collaborations and methods, Literary studies, Translation studies","<p>This workshop will illustrate the importance of translation alignment in the field of slow reading and language learning. We will provide a short theoretical overview on the principles of translation alignment, together with a hands-on tutorial on Ugarit (http://ugarit.ialigner.com/), a web-based translation alignment editor. Ugarit is designed as a Citizen Science tool, aiming at collecting training datasets of manually aligned words from diverse text corpora. The ultimate goal of Ugarit is to improve automatic translation alignment methods and to implement a set of dynamic lexica, with particular regard for languages with less supported infrastructures. However, the tool also has a strong pedagogical potential, which has been assessed in the course of various hands-on workshops and in an ongoing integration in school curricula: we have tested how translation alignment with Ugarit can help readers to engage with languages that they have never seen, grasping their essential semantic and morphological aspects. We propose text alignment as a way to empower the perception of the complexity of a language, but also as a method to leverage usual obstacles in the process of reading apparently “impenetrable” sources by directly engaging with them. At present, Ugarit includes aligned pairs from 36 languages (including less represented languages, like Bulgarian, Ethiopic, Sanskrit, Yiddish, and Armenian), 277 unique users, and about 23,400 parallel texts hosted.</p>
","This workshop will illustrate the importance of translation alignment in the field of slow reading and language learning. We will provide a short theoretical overview on the principles of translation alignment, together with a hands-on tutorial on Ugarit (http://ugarit.ialigner.com/), a web-based translation alignment editor. Ugarit is designed as a Citizen Science tool, aiming at collecting training datasets of manually aligned words from diverse text corpora. The ultimate goal of Ugarit is to improve automatic translation alignment methods and to implement a set of dynamic lexica, with particular regard for languages with less supported infrastructures. However, the tool also has a strong pedagogical potential, which has been assessed in the course of various hands-on workshops and in an ongoing integration in school curricula: we have tested how translation alignment with Ugarit can help readers to engage with languages that they have never seen, grasping their essential semantic and morphological aspects. We propose text alignment as a way to empower the perception of the complexity of a language, but also as a method to leverage usual obstacles in the process of reading apparently “impenetrable” sources by directly engaging with them. At present, Ugarit includes aligned pairs from 36 languages (including less represented languages, like Bulgarian, Ethiopic, Sanskrit, Yiddish, and Armenian), 277 unique users, and about 23,400 parallel texts hosted.","chiara.palladino@furman.edu, anna.zhang@furman.edu, maryam.foradi@uni-leipzig.de, tariq@informatik.uni-leipzig.de",Workshop/Tutorial 4
105,"Bordalejo, Barbara ; O'Donnell, Daniel Paul "," University of Saskatchewan, Canada;  University of Lethbridge, Canada",Diversity and Inclusion for Digital Humanists,Diversity and Inclusion for Digital Humanists,"Diversity, Inclusion, Bias, Privilege","Global, English, Contemporary, digital activism and advocacy, Disability and differently-abled studies, Feminist studies","<p>This workshop highlights issues of diversity, inclusivity, and collaboration in Digital Humanities.</p>
<p>Through practical exercises and dialogue, we build a safe atmosphere for the discussion of strategies to isolate obstacles preventing diversity and offer solutions for the development of inclusive environments.</p>
<p>As part of our work, we developed The Privilege Game, used to emphasize and showcase the many different kinds of privilege derived from our society's power structures and to create awareness among practitioners of the contrasts to be found in the rich and ever-growing space of the Digital Humanities.</p>
<p>We cover topics such as ""implicit bias,"" ""cultural cloning,"" privilege, intersectionality and solutions to be implemented in the creation of safe and inclusive environments.</p>
","This workshop highlights issues of diversity, inclusivity, and collaboration in Digital Humanities.
Through practical exercises and dialogue, we build a safe atmosphere for the discussion of strategies to isolate obstacles preventing diversity and offer solutions for the development of inclusive environments.
As part of our work, we developed The Privilege Game, used to emphasize and showcase the many different kinds of privilege derived from our society's power structures and to create awareness among practitioners of the contrasts to be found in the rich and ever-growing space of the Digital Humanities.
We cover topics such as ""implicit bias,"" ""cultural cloning,"" privilege, intersectionality and solutions to be implemented in the creation of safe and inclusive environments.","barbara.bordalejo@usask.ca, daniel.odonnell@uleth.ca",Workshop/Tutorial 4
106,"Boateng, Akwasi Bosompem","North-West University, South Africa",Social media in political engagements in Africa: A study of Twitter use in the intra-party elections of political parties in Ghana ,Social media in political engagements in Africa: A study of Twitter use in the intra-party elections of political parties in Ghana ,"Relationship Management, Elections, Political Engagement, Social Media, Twitter","Africa, English, Contemporary, social media analysis and methods, Communication studies, Political science","<p><em>The advent of social media is changing the dynamics of political communication, engagements and election campaigns across the world. These platforms extend opportunities for political actors especially political parties, governmenta and citizens to engage directly to build mutually beneficial relationships through engagements ans interactions. Using interviews and content analysis, this article explores “Twittering” in  political communication and engagements; how political parties use social media especially during their 2018 intra-party elections in Ghana. It examines how the New Patriotic Party and National Democratic Congress in Ghana use Twitter in intra-party elections. The article gathers data from political parties and their Twitter pages to theorize the political appropriation of social media in Ghana.The findings show that the political parties minimally appropriate Twitter; as they occasionally used it for public information purposes than interactive engagements and mutually beneficial relationships via two-way symmetrical communication. There were insignificant amount of tweet made by the political parties during their intra-party elections in 2018, which indicate that political use of social media in Ghana is still infantile. Therefore, political parties need to improve their use of Twitter and other social media platforms to take maxim advantage of the interactive features and relationship building potentials to garner support from the public and votes in elections. In this regard, political parties require enlightenEd communication strategies and professional political public relations tact for improved use to mature in social media communication.</em></p>
","The advent of social media is changing the dynamics of political communication, engagements and election campaigns across the world. These platforms extend opportunities for political actors especially political parties, governmenta and citizens to engage directly to build mutually beneficial relationships through engagements ans interactions. Using interviews and content analysis, this article explores “Twittering” in political communication and engagements; how political parties use social media especially during their 2018 intra-party elections in Ghana. It examines how the New Patriotic Party and National Democratic Congress in Ghana use Twitter in intra-party elections. The article gathers data from political parties and their Twitter pages to theorize the political appropriation of social media in Ghana.The findings show that the political parties minimally appropriate Twitter; as they occasionally used it for public information purposes than interactive engagements and mutually beneficial relationships via two-way symmetrical communication. There were insignificant amount of tweet made by the political parties during their intra-party elections in 2018, which indicate that political use of social media in Ghana is still infantile. Therefore, political parties need to improve their use of Twitter and other social media platforms to take maxim advantage of the interactive features and relationship building potentials to garner support from the public and votes in elections. In this regard, political parties require enlightenEd communication strategies and professional political public relations tact for improved use to mature in social media communication.",beebeeboateng@gmail.com,Poster
109,"Heintzman, Kit","Harvard University, Canada",Greening the Digital Humanities,Greening the Digital Humanities,"carbon emissions, environmentalism","Global, English, Contemporary, eco-criticism and environmental analysis, sustainable procedures, systems, and methods, History of science","<p>This workshop would introduce audiences to the following issues: recent historiography attention to the internet's carbon footprint, a case study in a semester long system of tracking the carbon emissions of a digital humanities course, trnasferable pedagogical strategies with regard to eco-conscious digital scholarshp, and critical reflections upon issues of ""individualism"" in discourse of digital consumption patterns versus system changes.</p>
","This workshop would introduce audiences to the following issues: recent historiography attention to the internet's carbon footprint, a case study in a semester long system of tracking the carbon emissions of a digital humanities course, trnasferable pedagogical strategies with regard to eco-conscious digital scholarshp, and critical reflections upon issues of ""individualism"" in discourse of digital consumption patterns versus system changes.",kheintzman@fas.harvard.edu,Workshop/Tutorial 2
110,"Ohya, Kazushi","Tsurumi University, Japan","An online course system easy to make, preserve, and promote critical thinking","An online course system easy to make, preserve, and promote critical thinking","radio lecture system, HTML Imports, Web Components","Global, English, Contemporary, data publishing projects, systems, and methods, sustainable procedures, systems, and methods, Education/ pedagogy, Media studies","<p>In this poster presentation we will show a new online course consisting of talks and chalks that is easy to make, edit, and preserve, and is a traditional and old-fashioned lecture style, that substantially helps students learn the content spontaneously. The course materials we need to make are only scripts for talks and XML data for slides, and the process we need to undertake separately is just recording the talk. The mechanisms to realize this course are backed with HTML5 and simple codes in JavaScript based on the specification HTML Imports. This lecture system is important not only for an implementation to realize a new teaching/learning channel conveying the knowledge instantly to learners but also to show the evidence for usefullness of HTML Imports as a sign of the existence of users' requirements.</p>
","In this poster presentation we will show a new online course consisting of talks and chalks that is easy to make, edit, and preserve, and is a traditional and old-fashioned lecture style, that substantially helps students learn the content spontaneously. The course materials we need to make are only scripts for talks and XML data for slides, and the process we need to undertake separately is just recording the talk. The mechanisms to realize this course are backed with HTML5 and simple codes in JavaScript based on the specification HTML Imports. This lecture system is important not only for an implementation to realize a new teaching/learning channel conveying the knowledge instantly to learners but also to show the evidence for usefullness of HTML Imports as a sign of the existence of users' requirements.",oya-k@tsurumi-u.ac.jp,Poster
111,"Calvo Tello, José",Göttingen State and University Library," What is a Genre? A Graph Unified Model of Categories, Texts, and Features "," What is a Genre? A Graph Unified Model of Categories, Texts, and Features ","genre, model, graph, novels, Spanish","Comparative (2 or more geographical areas), Europe, English, BCE-4th Century, 19th Century, 20th Century, cultural analytics, text mining and analysis, Literary studies","<p align=""justify"">Several theoretical models have been proposed for genre, such as the Aristotelian scholastic taxonomy, the family resemblance and the prototype theory. However, these models lack of empirical applications to real examples of genres. This proposal is the culmination of a series of analysis, presenting a theoretical, computational and visual graph-based model that fits several observations. This formalization unifies components of the previous theories, offering visually the intention (internal features) and extension (the best representatives and instances) of each category. Besides, it allows two intuitive interpretations based on the evaluation: the centrality as classification results, and the distance as similarity through shared features. The model is applied to three data-sets of different periods and languages: modern Spanish novels, classic French plays and the books of the Bible.</p>
","Several theoretical models have been proposed for genre, such as the Aristotelian scholastic taxonomy, the family resemblance and the prototype theory. However, these models lack of empirical applications to real examples of genres. This proposal is the culmination of a series of analysis, presenting a theoretical, computational and visual graph-based model that fits several observations. This formalization unifies components of the previous theories, offering visually the intention (internal features) and extension (the best representatives and instances) of each category. Besides, it allows two intuitive interpretations based on the evaluation: the centrality as classification results, and the distance as similarity through shared features. The model is applied to three data-sets of different periods and languages: modern Spanish novels, classic French plays and the books of the Bible.",jose.calvo@uni-wuerzburg.de,Long Presentation
112,"Hannesschläger, Vanessa; Wissik, Tanja","Austrian Centre for Digital Humanities, Austrian Academy of Sciences, Austria",Opening up Open Data: Strategies & success stories,Opening up Open Data: Strategies & success stories,"Open Data, Open Source, Hackathon, Community involvement","Global, Europe, English, Contemporary, open access methods, public humanities collaborations and methods, Humanities computing","<p dir=""ltr"">Not only in the context of Digital Humanities but also in other research areas the Open Data movement is gaining momentum. For this reason, the Austrian Centre for Digital Humanities of the Austrian Academy of Sciences (ACDH) started an experiment at the beginning of 2019: For the first time ever, we published the calls for participation in three virtual hackathons funded by the Austrian manifestations of DARIAH and CLARIN, CLARIAH-AT. These hackathons focused on Open Data sets that are publicly available online, and the tasks to perform on these data involved the creation of Open Source code. Each of the hackathons had a special theme and was co-timed with an event that involved an aspect of Openness. These events also inspired the choice of the respective data sets.</p>
<p dir=""ltr"">Usually hackathons take place on site, participants are given tasks to be solved within a given timeframe in a fixed location. This requires the programmers to be flexible and available and to have access to travel funding. A virtual hackathon on the other hand offers people all over the globe the possibility to participate and contribute without having to travel. Therefore, our approach enabled a much larger community to participate in the event on the one hand, thus also promoting the benefits of Open Data on the other.</p>
<p dir=""ltr"">The first hackathon was carried out in cooperation with the European Lexicographical Infrastructure (ELEXIS) and focused on lexicographical data (the Digital Dictionary of Tunis Arabic). The task was to develop a creative mode of processing it, e.g. by enriching it, visualizing it, doing statistical analysis, or integrating it with other resources (e.g. LOD). The submissions were to be handed in by the end of the first ELEXIS observer event taking place in February 2019.</p>
<p dir=""ltr"">For the second hackathon, the ACDH cooperated with the City of Vienna and chose a set from the city’s Open Government Data platform to be processed. The task was to be completed on the Open Data Day Vienna 2019 (28 February 2019). The aim was to develop a creative mode of processing cartographical data showing damage to buildings in Vienna during World War II.</p>
<p dir=""ltr"">The third and final hackathon of the series offered a task to be completed on the International Open Data Day 2019 (2 March 2019). For this final highlight, a choice of two Open Data sets was offered to the participants. The first data set to be worked on in this task was a collection of XML/TEI transcriptions of early German travel guides. The second data set consisted of German historical newspapers and was provided in cooperation with the Europeana newspaper project.</p>
<p dir=""ltr"">The best contributions were determined by an international board of judges and received cash prizes. The criteria for judgement were creativity and innovation, accessibility, reusability and reproducibility, as well as elegance. In our presentation, we will share the lessons learned and show how Open Science was the necessary precondition for this project, as well as what inspired its ultimate success. We will make our case by giving insights into the lessons learned in 2019 and sharing how we improved the concept for the second round of the hackathon series in 2020.</p>
<p dir=""ltr""><strong>Bibliography</strong></p>
<p dir=""ltr"">ACDH-CH. ACDH Virtual Hackathon Series. 2019. https://www.oeaw.ac.at/acdh/detail/event/acdh-virtual-hackathon-series/</p>
<p dir=""ltr"">ACDH-CH. ACDH-CH Open Data Virtual Hackathon - Round Two. 2020. https://www.oeaw.ac.at/acdh/detail/event/acdh-ch-open-data-virtual-hackathon-round-two/</p>
<p dir=""ltr"">Vanessa Hannesschläger. The ACDH virtual hackathon series: Open Data for Open Source solutions. DARIAH Open Blog, 23. May 2019. https://dariahopen.hypotheses.org/571</p>
","Not only in the context of Digital Humanities but also in other research areas the Open Data movement is gaining momentum. For this reason, the Austrian Centre for Digital Humanities of the Austrian Academy of Sciences (ACDH) started an experiment at the beginning of 2019: For the first time ever, we published the calls for participation in three virtual hackathons funded by the Austrian manifestations of DARIAH and CLARIN, CLARIAH-AT. These hackathons focused on Open Data sets that are publicly available online, and the tasks to perform on these data involved the creation of Open Source code. Each of the hackathons had a special theme and was co-timed with an event that involved an aspect of Openness. These events also inspired the choice of the respective data sets.
Usually hackathons take place on site, participants are given tasks to be solved within a given timeframe in a fixed location. This requires the programmers to be flexible and available and to have access to travel funding. A virtual hackathon on the other hand offers people all over the globe the possibility to participate and contribute without having to travel. Therefore, our approach enabled a much larger community to participate in the event on the one hand, thus also promoting the benefits of Open Data on the other.
The first hackathon was carried out in cooperation with the European Lexicographical Infrastructure (ELEXIS) and focused on lexicographical data (the Digital Dictionary of Tunis Arabic). The task was to develop a creative mode of processing it, e.g. by enriching it, visualizing it, doing statistical analysis, or integrating it with other resources (e.g. LOD). The submissions were to be handed in by the end of the first ELEXIS observer event taking place in February 2019.
For the second hackathon, the ACDH cooperated with the City of Vienna and chose a set from the city’s Open Government Data platform to be processed. The task was to be completed on the Open Data Day Vienna 2019 (28 February 2019). The aim was to develop a creative mode of processing cartographical data showing damage to buildings in Vienna during World War II.
The third and final hackathon of the series offered a task to be completed on the International Open Data Day 2019 (2 March 2019). For this final highlight, a choice of two Open Data sets was offered to the participants. The first data set to be worked on in this task was a collection of XML/TEI transcriptions of early German travel guides. The second data set consisted of German historical newspapers and was provided in cooperation with the Europeana newspaper project.
The best contributions were determined by an international board of judges and received cash prizes. The criteria for judgement were creativity and innovation, accessibility, reusability and reproducibility, as well as elegance. In our presentation, we will share the lessons learned and show how Open Science was the necessary precondition for this project, as well as what inspired its ultimate success. We will make our case by giving insights into the lessons learned in 2019 and sharing how we improved the concept for the second round of the hackathon series in 2020.
Bibliography
ACDH-CH. ACDH Virtual Hackathon Series. 2019. https://www.oeaw.ac.at/acdh/detail/event/acdh-virtual-hackathon-series/
ACDH-CH. ACDH-CH Open Data Virtual Hackathon - Round Two. 2020. https://www.oeaw.ac.at/acdh/detail/event/acdh-ch-open-data-virtual-hackathon-round-two/
Vanessa Hannesschläger. The ACDH virtual hackathon series: Open Data for Open Source solutions. DARIAH Open Blog, 23. May 2019. https://dariahopen.hypotheses.org/571","vanessa.hannesschlaeger@oeaw.ac.at, tanja.wissik@oeaw.ac.at",Lightning
113,"SAIBU, ISRAEL ABAYOMI ; SAIBU, AYOMIDE JOSEPH "," ANCHOR UNIVERSITY, LAGOS NIGERIA.;  LAGOS STATE UNIVERSITY OJO, LAGOS NIGERIA",PRESERVATION OF OSUN-OSOGBO CULTURAL HERITAGE IN NIGERIA: A DIGITIZATION DISCOURSE,PRESERVATION OF OSUN-OSOGBO CULTURAL HERITAGE IN NIGERIA: A DIGITIZATION DISCOURSE,"Preservation, Digitization, Cultural Heritage, Osun-Osogbo, Nigeria","Africa, English, 19th Century, Contemporary, digital archiving, digitization (2D & 3D), Art history, History","<p>The Osun-Osogbo cultural heritage has become one of the most striking cultural identities to have emerged in Nigeria. The uniqueness and importance of Osun-Osogbo has led to its recognition by the United Nations Educational Scientific and Cultural Organization (UNESCO) as a global cultural heritage. Thus, it is imperative that this cultural heritage be digitized for the preservation of its essence for posterity and global visibility.</p>
","The Osun-Osogbo cultural heritage has become one of the most striking cultural identities to have emerged in Nigeria. The uniqueness and importance of Osun-Osogbo has led to its recognition by the United Nations Educational Scientific and Cultural Organization (UNESCO) as a global cultural heritage. Thus, it is imperative that this cultural heritage be digitized for the preservation of its essence for posterity and global visibility.","alerosaibu@gmail.com, ayomidesaibu2001@gmail.com",Lightning
114,"Herold, Nastasia (1,2); Ottawa, Thérèse "," University of Leipzig, Germany;  Wikipetcia Atikamekw Nehiromowin, Canada",Ethics and responsibilities of open access - lessons learned from the Wikipedia project of the Atikamekw First Nation,Ethics and responsibilities of open access - lessons learned from the Wikipedia project of the Atikamekw First Nation,"open access, wikipedia, atikamekw, first nations, collaborative","English, North America, Contemporary, digital access, privacy, and ethics analysis, open access methods, First nations and indigenous studies, Linguistics","<p align=""center"">Abstract by Nastasia Herold & Thérèse Ottawa<u></u></p>
<p align=""center""><u> </u></p>
<p align=""center""><u>Ethics and responsibilities of open access – lessons learned from the Wikipedia project of the Atikamekw First Nation</u></p>
<p><u> </u></p>
<p>With 97,9% (cf. INAC 2019), in Canada, the Atikamekw First Nation has the highest percentage of people who speak their native language (Atikamekw) at home. The Atikamekw live in Quebec, Canada, and have a population of 8,000 people in three communities.</p>

<p>In 2013, Nastasia Herold, one of the authors of this paper did a field study in Manawan, one of the three Atikamekw communities, for a research on the local bilingualism (Atikamekw and French). Despite the vitality of the Atikamekw language, a survey and interviews showed that francization and language change are processes noticed by all living generations of the Atikamekw (cf. Herold 2020: N.N.).</p>

<p>Communication takes place more and more often digitally (cf. Reichert 2017: 26-27), and this in written language rather than orally. Atikamekw has a standardized orthography since 1994 (cf. Dinnison <sup>2</sup>1997) and is taught in Manawan’s primary school as a first language and as medium of alphabetization. However, Herold’s (2019: 103) research in 2013 showed that the Internet contained no written text in the Atikamekw language, the Atikamekw used the Internet mainly in French. This is why a school project at Manawan’s secondary school was initiated in 2013 in order to create a Wikipedia site in the Atikamekw language.</p>

<p>Many lessons have been learned during the collaboration of academics, teachers, pupils, local language experts and other local voluntary contributors. The lessons we would like to focus on in this presentation are the lessons learned when implementing cultural knowledge to an open access platform. Open access is the free provision of (scientific) texts on the web without restriction of use (cf. Kohle 2017: 203), and this free provision has advantages and disadvantages.</p>

<p>We will give four examples which show that the Atikamekw made sure that the knowledge was published respecting their own principles, beliefs and tradition. These four examples and the development of the project show how it is important that the community itself dictates rules that have to be respected when publishing their knowledge under open access.</p>

<p>Finally we will answer Rehbein’s and Thies’ (2017: 355) question they developed as a schema for questions of responsibilities and ethics of a specific project: Who (1) is responsible for what (2) to whom (3) before which instance (4) according to which standards (5)?</p>

<p><u>References</u></p>
<p>Dinnison, Bonnie (<sup>2</sup>1997): <em>Guide orthographique de la langue atikamekw</em>. La Tuque: Atikamekw Sipi.</p>
<p>Herold, Nastasia (2019): “Kulturbedingte Herausforderungen für schulische und gesellschaftliche Teilhabe in indigenen Reservaten Kanadas: Das Beispiel des Atikamekw-Dorfes Manawan in Québec”, in: Jahr, David / Kruschel, Robert (eds.): <em>Inklusion in Kanada</em>. Internationale Perspektiven auf heterogenitätssensible Bildung. Weinheim: Beltz Juventa 92-106.</p>
<p>Herold, Nastasia (2020): “La situation linguistique bilingue dans la communauté atikamekw de Manawan – La consolidation d’une langue minoritaire”, in: <em>Recherches amérindiennes au Québec</em>, N.N. [submitted].</p>
<p>INAC (26/09/2019): <em>Tribal Council Detail</em>. Atikamekw Sipi – Conseil de la Nation Atikamekw <http://fnp-ppn.aandc-aadnc.gc.ca/fnp/Main/Search/TCMain.aspx?TC_NUMBER=1064&lang=eng> (01/10/2019).</p>
<p>Kohle, Hubertus (2017): “Digitales Publizieren”, in: Jannidis, Fotis / Kohle, Hubertus / Rehbein, Malte (eds.): <em>Digital Humanities</em>. Eine Einführung. Stuttgart: J.B. Metzler 199-205.</p>
<p>Rehbein, Malte / Thies, Christian (2017): “Ethik”, in: Jannidis, Fotis / Kohle, Hubertus / Rehbein, Malte (eds.): <em>Digital Humanities</em>. Eine Einführung. Stuttgart: J.B. Metzler 353-357.</p>
<p>Reichert, Ramón (2017): “Theorien digitaler Medien”, in: Jannidis, Fotis / Kohle, Hubertus / Rehbein, Malte (eds.): <em>Digital Humanities</em>. Eine Einführung. Stuttgart: J.B. Metzler 19-34.</p>
","Abstract by Nastasia Herold & Thérèse Ottawa
 
Ethics and responsibilities of open access – lessons learned from the Wikipedia project of the Atikamekw First Nation
 
With 97,9% (cf. INAC 2019), in Canada, the Atikamekw First Nation has the highest percentage of people who speak their native language (Atikamekw) at home. The Atikamekw live in Quebec, Canada, and have a population of 8,000 people in three communities.
In 2013, Nastasia Herold, one of the authors of this paper did a field study in Manawan, one of the three Atikamekw communities, for a research on the local bilingualism (Atikamekw and French). Despite the vitality of the Atikamekw language, a survey and interviews showed that francization and language change are processes noticed by all living generations of the Atikamekw (cf. Herold 2020: N.N.).
Communication takes place more and more often digitally (cf. Reichert 2017: 26-27), and this in written language rather than orally. Atikamekw has a standardized orthography since 1994 (cf. Dinnison 21997) and is taught in Manawan’s primary school as a first language and as medium of alphabetization. However, Herold’s (2019: 103) research in 2013 showed that the Internet contained no written text in the Atikamekw language, the Atikamekw used the Internet mainly in French. This is why a school project at Manawan’s secondary school was initiated in 2013 in order to create a Wikipedia site in the Atikamekw language.
Many lessons have been learned during the collaboration of academics, teachers, pupils, local language experts and other local voluntary contributors. The lessons we would like to focus on in this presentation are the lessons learned when implementing cultural knowledge to an open access platform. Open access is the free provision of (scientific) texts on the web without restriction of use (cf. Kohle 2017: 203), and this free provision has advantages and disadvantages.
We will give four examples which show that the Atikamekw made sure that the knowledge was published respecting their own principles, beliefs and tradition. These four examples and the development of the project show how it is important that the community itself dictates rules that have to be respected when publishing their knowledge under open access.
Finally we will answer Rehbein’s and Thies’ (2017: 355) question they developed as a schema for questions of responsibilities and ethics of a specific project: Who (1) is responsible for what (2) to whom (3) before which instance (4) according to which standards (5)?
References
Dinnison, Bonnie (21997): Guide orthographique de la langue atikamekw. La Tuque: Atikamekw Sipi.
Herold, Nastasia (2019): “Kulturbedingte Herausforderungen für schulische und gesellschaftliche Teilhabe in indigenen Reservaten Kanadas: Das Beispiel des Atikamekw-Dorfes Manawan in Québec”, in: Jahr, David / Kruschel, Robert (eds.): Inklusion in Kanada. Internationale Perspektiven auf heterogenitätssensible Bildung. Weinheim: Beltz Juventa 92-106.
Herold, Nastasia (2020): “La situation linguistique bilingue dans la communauté atikamekw de Manawan – La consolidation d’une langue minoritaire”, in: Recherches amérindiennes au Québec, N.N. [submitted].
INAC (26/09/2019): Tribal Council Detail. Atikamekw Sipi – Conseil de la Nation Atikamekw <http://fnp-ppn.aandc-aadnc.gc.ca/fnp/Main/Search/TCMain.aspx?TC_NUMBER=1064&lang=eng> (01/10/2019).
Kohle, Hubertus (2017): “Digitales Publizieren”, in: Jannidis, Fotis / Kohle, Hubertus / Rehbein, Malte (eds.): Digital Humanities. Eine Einführung. Stuttgart: J.B. Metzler 199-205.
Rehbein, Malte / Thies, Christian (2017): “Ethik”, in: Jannidis, Fotis / Kohle, Hubertus / Rehbein, Malte (eds.): Digital Humanities. Eine Einführung. Stuttgart: J.B. Metzler 353-357.
Reichert, Ramón (2017): “Theorien digitaler Medien”, in: Jannidis, Fotis / Kohle, Hubertus / Rehbein, Malte (eds.): Digital Humanities. Eine Einführung. Stuttgart: J.B. Metzler 19-34.","nastasia.herold@uni-leipzig.de, thereseottawa@gmail.com",Short Presentation
116,"Shibutani, Ayako ; Goto, Makoto "," The University Museum, the University of Tokyo, Japan;  National Museum of Japanese History, Japan",How Do Research Data Develop? International Standardisation of Scientific Data in Historical Studies,How Do Research Data Develop? International Standardisation of Scientific Data in Historical Studies,"Cultural Heritage Science, Japanese history, linked data, open data, standardization","Asia, English, 15th-17th Century, 18th Century, 19th Century, digital research infrastructures development and analysis, open access methods, History, History of science","<p>The scientific approach to historical resources creates a synthetic discipline benefitting from open access to data from the humanities and sciences. However, technological challenges exist because of dispersed and heterogeneous resource data. Through data sharing of historical resources, this paper proposes that the establishment of integrated data repositories will capture data provenance and diversity while promoting attribution and acknowledgment of its use. TTo enhance and accelerate scientific advances in historical studies, we enumerate digital humanities applications to solve the technological and sociological challenges that have limited open access to resource data in the world. We also standardise the scientific methodology of historical materials research using the following approaches: a qualitative analysis focusing on component details to compare our findings with the classifications granted to historical materials in previous studies, and the reconstruction of papermaking methods using DNA analysis.</p>
","The scientific approach to historical resources creates a synthetic discipline benefitting from open access to data from the humanities and sciences. However, technological challenges exist because of dispersed and heterogeneous resource data. Through data sharing of historical resources, this paper proposes that the establishment of integrated data repositories will capture data provenance and diversity while promoting attribution and acknowledgment of its use. TTo enhance and accelerate scientific advances in historical studies, we enumerate digital humanities applications to solve the technological and sociological challenges that have limited open access to resource data in the world. We also standardise the scientific methodology of historical materials research using the following approaches: a qualitative analysis focusing on component details to compare our findings with the classifications granted to historical materials in previous studies, and the reconstruction of papermaking methods using DNA analysis.","ashibutani@hi.u-tokyo.ac.jp, m-goto@rekihaku.ac.jp",Poster
117,"Licastro, Amanda Marie ; Skallerup Bessette, Lee Elaine ; Whalen, Zachary N ; McGrail, Anne B. "," Stevenson University, United States of America;  Georgetown University;  University of Mary Washington;  Lane Community College, Eugene, Oregon US",Exploring the Undiscovered Contours of DH,Exploring the Undiscovered Contours of DH,"pedagogy, marginalization, digital studies, infrastructure, curriculum","English, North America, Contemporary, curricular and pedagogical development and analysis, digital activism and advocacy, Education/ pedagogy","<p>How can scholars on the margins of DH articulate their work in DH publications, grants, and other professional and disciplinary outlets? In this interactive forum, we aim to explore inclusion--or exclusion--in what counts as “digital humanities” among scholars across disciplines, institutional contexts, and employment statuses. We begin by surveying audience members about the alternative ways that they represent their digital work and their different institutional contexts. We then ask participants to explore how esoteric terms such as “digital humanities” may be illegible to administrators and the public and the effects of this illegibility on their pedagogy and professional work. After collectively articulating the problem as it stands today, forum leaders will facilitate a problem-solving conversation that might begin addressing the issue.</p>
","How can scholars on the margins of DH articulate their work in DH publications, grants, and other professional and disciplinary outlets? In this interactive forum, we aim to explore inclusion--or exclusion--in what counts as “digital humanities” among scholars across disciplines, institutional contexts, and employment statuses. We begin by surveying audience members about the alternative ways that they represent their digital work and their different institutional contexts. We then ask participants to explore how esoteric terms such as “digital humanities” may be illegible to administrators and the public and the effects of this illegibility on their pedagogy and professional work. After collectively articulating the problem as it stands today, forum leaders will facilitate a problem-solving conversation that might begin addressing the issue.","amanda.licastro@gmail.com, ls1335@georgetown.edu, zwhalen@umw.edu, mcgraila@lanecc.edu",Forum
118,"Ma, Rongqian","University of Pittsburgh, United States of America",CR/10 Website as a Digital Public Humanities (DPH) Site,CR/10 Website as a Digital Public Humanities (DPH) Site,"Public digital humanities, user experience design, CR/10, cultural revolution","Asia, English, North America, 20th Century, Contemporary, public humanities collaborations and methods, user experience design and analysis, Design studies, Library & information science","<p>Initiated by Mao Zedong (1893-1976) and lasting from 1966 to 1976, the Cultural Revolution in China has been defined as a 10-year disaster by the Chinese Communist Party (CCP), which caused civil unrest to the party, the state, and the people (CCP, 1981). Forty years after the end of the Cultural Revolution, with the intention to promote the public remembrance and discussion of this significant period of Chinese history, the East Asian Library at the University of Pittsburgh launched CR/10 (University of Pittsburgh, 2019), a digital oral history project that aims to collect and preserve authentic memories of the Cultural Revolution through 10-minute semi-structured interviews. Using the technique of snowball sampling for interviewee selection, the project has thus far collected more than 300 interviews with ordinary people from different generations, geographies, social and educational backgrounds who experienced the incident or learned about it from family, school, or other resources. Among all the interviewees, some currently live in the U.S. while some in China, whose everyday experiences post the Cultural Revolution may have exerted impacts on their individual memories. Most interviews were conducted in mandarin Chinese and were then translated into English as subtitles by the CR/10 team. All the interviews were video-recorded and made open-access on the interactive CR/10 website. The website consists of four major components: an introduction to the project, a trailer for the project that showcases interviews, a timeline and a map of China demonstrating the temporal and geographical distributions of the videos in the collection.</p>
<p>In this paper, I treat the CR/10 website as the object of study and demonstrate its value as a digital public humanities site bridging the gap between the academy and the general public. Cox and Tilton (2019) defined the term “digital public humanities (DPH)” as practices that “facilitate reflection and collaboration with participants outside of the academy through digital theories and technologies” (p. 130). Focusing on interactive and mindful design (Drucker, 2013), the CR/10 website invites the public to participate in, as well as contributing to, developing a diversified and multifaceted understanding of the Cultural Revolution. In this study, I present a series of user experience research conducted regarding the design of the website with 15 users outside of the academy, to examine the current usability as well as to identify further design possibilities of the website. Through techniques of semi-structured contextual interviews and focus group study, this research study aims to reflect on the reception and use of the CR/10 website as an interactive teaching and learning platform, rather than solely a repository of collective memories or database of historical information. More specifically, I examine how the timeline and map features of the website enrich interpretations of the Cultural Revolution, especially in terms of inspiring users to reflect upon the following questions: How could the Cultural Revolution be defined? What factors (e.g., geography, generation, family, class backgrounds, and education) influence impressions and memories of the Cultural Revolution? Extending from the user experience research, this study also proposes recommendations to improve the design of the CR/10 website to create a collective “memory atlas” (Cornell University Library, 2013; Forster, 1976) out of the video collection. Findings of this study contribute to facilitating academy-public collaborations in building DPH sites from the user perspective and a design-mediated approach.</p>
<p>References</p>
<p>Chinese Communist Party (CCP). (1981). Resolution on certain questions in the history of our Party since the founding of the People's Republic of China. Retrieved from http://en.people.cn/dengxp/vol2/text/b1420.html</p>
<p>Cornell University Library. (2013). <em>Mnemosyne: Meanderings through Aby Warburg’s Atlas</em>. Retrieved from https://warburg.library.cornell.edu/</p>
<p>Cox, J., & Tilton, L. (2019). The digital public humanities: Giving new arguments and new ways to argue. <em>Review of Communication 19</em> (2), 127-146.</p>
<p>Drucker, J. (2013). Reading interface. <em>PMLA 128 </em>(1), 213-220.</p>
<p>Forster, K. (1976). Aby Warburg’s History of Art: Collective Memory and the Social Mediation of Images. <em>Daedalus 105 </em>(1), 169-176.</p>
<p>University of Pittsburgh. (2019). <em>CR/10</em>. Retrieved from http://www.culturalrevolution.pitt.edu/.</p>
","Initiated by Mao Zedong (1893-1976) and lasting from 1966 to 1976, the Cultural Revolution in China has been defined as a 10-year disaster by the Chinese Communist Party (CCP), which caused civil unrest to the party, the state, and the people (CCP, 1981). Forty years after the end of the Cultural Revolution, with the intention to promote the public remembrance and discussion of this significant period of Chinese history, the East Asian Library at the University of Pittsburgh launched CR/10 (University of Pittsburgh, 2019), a digital oral history project that aims to collect and preserve authentic memories of the Cultural Revolution through 10-minute semi-structured interviews. Using the technique of snowball sampling for interviewee selection, the project has thus far collected more than 300 interviews with ordinary people from different generations, geographies, social and educational backgrounds who experienced the incident or learned about it from family, school, or other resources. Among all the interviewees, some currently live in the U.S. while some in China, whose everyday experiences post the Cultural Revolution may have exerted impacts on their individual memories. Most interviews were conducted in mandarin Chinese and were then translated into English as subtitles by the CR/10 team. All the interviews were video-recorded and made open-access on the interactive CR/10 website. The website consists of four major components: an introduction to the project, a trailer for the project that showcases interviews, a timeline and a map of China demonstrating the temporal and geographical distributions of the videos in the collection.
In this paper, I treat the CR/10 website as the object of study and demonstrate its value as a digital public humanities site bridging the gap between the academy and the general public. Cox and Tilton (2019) defined the term “digital public humanities (DPH)” as practices that “facilitate reflection and collaboration with participants outside of the academy through digital theories and technologies” (p. 130). Focusing on interactive and mindful design (Drucker, 2013), the CR/10 website invites the public to participate in, as well as contributing to, developing a diversified and multifaceted understanding of the Cultural Revolution. In this study, I present a series of user experience research conducted regarding the design of the website with 15 users outside of the academy, to examine the current usability as well as to identify further design possibilities of the website. Through techniques of semi-structured contextual interviews and focus group study, this research study aims to reflect on the reception and use of the CR/10 website as an interactive teaching and learning platform, rather than solely a repository of collective memories or database of historical information. More specifically, I examine how the timeline and map features of the website enrich interpretations of the Cultural Revolution, especially in terms of inspiring users to reflect upon the following questions: How could the Cultural Revolution be defined? What factors (e.g., geography, generation, family, class backgrounds, and education) influence impressions and memories of the Cultural Revolution? Extending from the user experience research, this study also proposes recommendations to improve the design of the CR/10 website to create a collective “memory atlas” (Cornell University Library, 2013; Forster, 1976) out of the video collection. Findings of this study contribute to facilitating academy-public collaborations in building DPH sites from the user perspective and a design-mediated approach.
References
Chinese Communist Party (CCP). (1981). Resolution on certain questions in the history of our Party since the founding of the People's Republic of China. Retrieved from http://en.people.cn/dengxp/vol2/text/b1420.html
Cornell University Library. (2013). Mnemosyne: Meanderings through Aby Warburg’s Atlas. Retrieved from https://warburg.library.cornell.edu/
Cox, J., & Tilton, L. (2019). The digital public humanities: Giving new arguments and new ways to argue. Review of Communication 19 (2), 127-146.
Drucker, J. (2013). Reading interface. PMLA 128 (1), 213-220.
Forster, K. (1976). Aby Warburg’s History of Art: Collective Memory and the Social Mediation of Images. Daedalus 105 (1), 169-176.
University of Pittsburgh. (2019). CR/10. Retrieved from http://www.culturalrevolution.pitt.edu/.",rom77@pitt.edu,Short Presentation
119,"Van Zundert, Joris J. ; Mar, Raymond A. ; van Dalen–Oskam, Karina (1,3); Temple, Emily ; Bowman, Isabel ; Heidari, Farzaneh ; Nguyen, Ahn T.P. "," Department of Literary Studies, Huygens Institute for the History of the Netherlands – Royal Netherlands Academy of Arts and Sciences. Amsterdam, The Netherlands;  Department of Psychology, York University. Toronto, Canada;  Faculty of Humanities, University of Amsterdam. Amsterdam, The Netherlands;  Literary Hub;  Department of Psychology, University of Toronto. Toronto, Canada;  Department of Electrical Engineering and Computer Sciences, York University. Toronto, Canada",Features of Timelessness: Intermediate Report on a Quest for Stylistic Features that Mark Literary Canonicity,Features of Timelessness: Intermediate Report on a Quest for Stylistic Features that Mark Literary Canonicity,"stylometry, literature, features, canonicity","Europe, English, North America, 20th Century, Contemporary, attribution studies and stylometric analysis, Humanities computing, Literary studies","<p>We report on our ongoing quest to establish a validated complex of stylistic features that act as markers for literary canonicity, in specific contexts. Currentely we present a stylometric analysis of literature investigating the stylistic markers that differentiate former bestsellers from fiction that remains popular across several decades using a TfIdf vectorization of texts and UMAP dimenision reduction approach. We find that especially a greater variation in sentence length is associated with the chances of a novel to remain popular.</p>
","We report on our ongoing quest to establish a validated complex of stylistic features that act as markers for literary canonicity, in specific contexts. Currentely we present a stylometric analysis of literature investigating the stylistic markers that differentiate former bestsellers from fiction that remains popular across several decades using a TfIdf vectorization of texts and UMAP dimenision reduction approach. We find that especially a greater variation in sentence length is associated with the chances of a novel to remain popular.","joris.van.zundert@huygens.knaw.nl, mar@yorku.ca, karina.van.dalen@huygens.knaw.nl, etemple@lithub.com, isabel.bowman@mail.utoronto.ca, farzanah@cse.yorku.ca, ntpanh1602@gmail.com",Short Presentation
120,"Schildkamp, Philip ; Harzenetter, Lukas ; Leymann, Frank ; Mathiak, Brigitte ; Neuefeind, Claes ; Breitenbücher, Uwe "," Data Center for the Humanities, University of Cologne, Germany;  Institute of Architecture of Application Systems, University of Stuttgart, Germany;  Cologne Center for eHumanities, University of Cologne, Germany;  University of Stuttgart, Germany",Workshop on Modelling and Maintaining Research Applications in TOSCA,Workshop on Modelling and Maintaining Research Applications in TOSCA,"Living Systems, Research Applications, Software Stacks, Sustainability, TOSCA","Global, English, Contemporary, software development, systems, analysis and methods, sustainable procedures, systems, and methods, Humanities computing, Informatics","<p dir=""ltr""><strong>Abstract</strong></p>
<p dir=""ltr"">The project ""SustainLife – Sustaining Living Digital Systems in the Humanities"" that is currently running at the Institute of Architecture of Application Systems (IAAS, University of Stuttgart) and the Data Center for the Humanities (DCH, University of Cologne) deals with the conservation of research applications in the field of Digital Humanities (DH). By employing the TOSCA standard (Topology and Orchestration Specification for Cloud Applications) to fully automate the deployment of DH applications and to keep them available in the long term, we try to tackle the problem of software obsolescence in the field of DH. To interactively demonstrate our approach to the international DH community, we would like to give a workshop on the topic ""Modelling and Maintaining Research Applications in TOSCA"" in the run-up to the DH 2020 conference. Thereinwe will show how to model (DH) software systems with TOSCA and share experiences and best practices on how to work with the OpenTOSCA ecosystem, an open-source implementation of the TOSCA standard.</p>
<p dir=""ltr""><strong></strong></p>
<p dir=""ltr""><strong>The Problem</strong></p>
<p dir=""ltr"">The establishment of the DH as an independent scientific research area as well as the increasing usage of digital methods in the research process require adjustments to common result assurance practices. For example, the long-term archiving (LTA) of primary research data uses well-established practices such as employing standardized data formats and forwarding data to permanent repositories. However, the fact that digital artifacts generated in DH-oriented research do not only consist of primary data but also contain research software is mostly disregarded (Sahle and Kronenwett, 2013). Moreover, the variety of DH research outcomes includes so-called ""living systems"" in which the software to present, access or analyze the data represents an essential part of the actual research output (Bingert et al., 2016). In contrast to classical research results such as monographs or encyclopedias, living systems cannot be served long-term without maintenance as their instantiation, supervision, and permanent provisioning represent major technical, organizational, and financial challenges. Furthermore, the heterogeneity of the research software generated in the DH requires a highly flexible preservation strategy, i.e., a suitable technology that ensures standardization, reusability, and archiving of as many digital artifacts as possible (Barzen et al., 2018). In addition to the aforementioned challenges, i.e., heterogeneity, underfunding, and obsolescence of digital artifacts, scientific practice requires long-term interoperability and traceability of all research outcomes. With regard to digital systems, these requirements are (1) constant accessibility, (2) the possibility of error-free operation, and (3) the ability to reconstruct any stage of development of a research application at any time without major structural difficulties.</p>
<p dir=""ltr""><strong></strong></p>
<p dir=""ltr""><strong>Our Approach</strong></p>
<p dir=""ltr"">The TOSCA standard (OASIS, 2013 and 2019) allows software systems to be modelled, provisioned, and deployed in a standardized and provider-independent manner. Thus, it is suitable for long-term archiving and operation of research applications produced within the field of DH (Neuefeind et al., 2018 and 2019). Following the TOSCA standard, applications are modelled in “Topology Templates” by describing their components and their relations amongst each other: Components are represented as “Node Templates”, while relations are modeled as “Relationship Templates”. Moreover, the semantics of a Node Template or Relationship Template are dictated by reusable types, i.e., “Node Types” and “Relationship Types” respectively. For example, a Python web application can be modelled as a Node Template that is an instance of the ""Python Application"" Node Type. To express that the Python Application accesses a MySQL database, a second Node Template that is of type ""MySQL Database"" can be added to the Topology Template. Then, the connection between both components can be described by a Relationship Template that is an instance of the Relationship Type ""connectsTo"". Additionally, to specify that both components are running on an Ubuntu virtual machine (VM), a Node Template of type ""Ubuntu VM"" can be added, while Relationship Templates of type “hostedOn” between the Python Application Node Template and the VM Node Template, as well as between the MySQL Database and the VM describe their respective hosting relations.</p>
<p dir=""ltr"">Hereby, TOSCA's type system enables the modelling of reusable component types, e.g., the ""Python Application"" Node Type, which can be reused in multiple Topology Templates describing different applications. Therefore, synergic effects emerge as existing Node Types can be reused in other Topology Templates, easing the modelling of new applications. In addition, the open-source TOSCA implementation OpenTOSCA (Breitenbücher et al., 2016) offers the possibility to graphically model applications using the TOSCA editor “Winery” (Kopp et al., 2013) which further simplifies the creation of new applications by providing drag-and-drop modeling capabilities.</p>
<p dir=""ltr""><strong></strong></p>
<p dir=""ltr""><strong>Workshop Curriculum</strong></p>
<p dir=""ltr"">During our four hour workshop, we will (1) give an overview to different solutions for long-term preservation of living systems and (2) describe the modeling language TOSCA. Based on these theoretical units, practical tasks will introduce (3) the modelling of an existing application using TOSCA and (4) how applications can be deployed using the OpenTOSCA ecosystem. Thus, by combining the theoretical foundations and the practical application of TOSCA, the participants will be able to model (research) software systems according to the standard and provision and deploy applications using the OpenTOSCA ecosystem.</p>
<p dir=""ltr"">The practical tasks are structured as follows: (1) Identify the components of an application and (2) describe them and their relations among each other in an TOSCA-based application topology, i.e., in a Topology Template. By fragmenting an application into its components and mapping them to TOSCA Node Types, the Topology Template describing the application can then be modelled using the OpenTOSCA ecosystem. Afterwards (3), the modelled TOSCA application will be deployed by the OpenTOSCA runtime. Moreover, by sharing our experiences and best practices in using OpenTOSCA with the community, we will introduce concepts such as ""software stacks"" in a practical way.</p>
<p dir=""ltr""><strong></strong></p>
<p dir=""ltr""><strong>Target Group</strong></p>
<p dir=""ltr"">The workshop is primarily designed for data center employees, libraries and other institutions focusing on infrastructures for long-term archiving and operation of heterogeneous software systems. Previous experience in dealing with Linux and writing shell scripts as well as with software stacks and service orchestration are helpful but not necessary for a successful participation. To provide a productive context for communicating the described content and to enable individual consultation and support, we designed the workshop for about 20 participants but limit it to a maximum of 30 participants.</p>
<p dir=""ltr""><strong></strong></p>
<p dir=""ltr""><strong>Technical Prerequisites</strong></p>
<p dir=""ltr"">For a successful participation in the workshop, it is necessary that each participant brings his/her own laptop. Although a shared instance of the OpenTOSCA ecosystem will be provided, it is desirable that all participants set up an OpenTOSCA instance on their work equipment prior to the workshop in order to perform modelling and deployment tasks on their own devices. Therefore, registered participants will be provided with all necessary information about system requirements and how to setup OpenTOSCA prior to the workshop. Furthermore, relevant documentation, publications, and manuals will be provided both in advance and in the context of the workshop. In addition, a stable internet connection as well as a sufficient number of power outlets for all electronic devices are indispensable. </p>
<p dir=""ltr""><br /><strong>About the Instructors</strong></p>
<p dir=""ltr"">Uwe Breitenbücher is a research staff member and postdoc at the Institute of Architecture of Application Systems (IAAS) at the University of Stuttgart, Germany. His research vision is to improve cloud application provisioning and application management by automating the application of management patterns. Uwe was part of the CloudCycle project, in which the OpenTOSCA Ecosystem was developed. His current research interests include cyber-physical systems, blockchains, and microservices.</p>
<p dir=""ltr"">Anna Fischer is a research assistant at the Data Center for the Humanities (DCH) at the University of Cologne and joined the “SustainLife” Project in January 2020. Her recent research and working activities have focused on data management and software development for natural language processing tasks, e.g., in collaboration with one of the chairs for Romance linguistics at the University of Cologne. </p>
<p dir=""ltr"">Lukas Harzenetter is a research associate at the Institute of Architecture of Application Systems (IAAS) at the University of Stuttgart, Germany. He received his Master of Science degree from the University of Stuttgart in Software Engineering in 2018. His research interests are in the field of cloud deployment and management models focusing on the development and change of such models over time. Lukas is part of the “SustainLife” project which is working on sustainable application deployments in the domain of digital humanities.</p>
<p dir=""ltr"">Frank Leymann is a full professor of computer science and director of the Institute of Architecture of Application Systems (IAAS) at the University of Stuttgart, Germany. His research interests include service-oriented architectures and associated middleware, workflow- and business process management, cloud computing and associated systems management aspects, and patterns. Frank is co-author of more than 400 peer-reviewed papers, about 70 patents, and several industry standards. He is an elected member of the Academy of Europe.</p>
<p dir=""ltr"">Brigitte Mathiak is chairwoman of the Data Center for the Humanities (DCH) and is particularly interested in data management and text mining. The idea for the ""SustainLife"" project arose after she had experienced again and again how living systems have to be abandoned or neglected. She is Junior Professor for Digital Humanities at the University of Cologne and Senior Scientist at the Leibniz Institute for the Social Sciences (GESIS).</p>
<p dir=""ltr"">Claes Neuefeind is a postdoc at the Cologne Center for eHumanities (CCeH) at the University of Cologne. He worked with Philip Schildkamp and Lukas Harzenetter on the DFG-LIS project ""SustainLife"" until October 2019 and changed for a position that is responsible for coordinating the Digital Humanities of the North Rhine-Westphalian Academy of Sciences and the Arts office.</p>
<p dir=""ltr"">Philip Schildkamp has been researching since 2015 and teaching since 2017 at the University of Cologne. He studied sociology, psychology, and Digital Humanities information processing. The main topics of his employment are technical infrastructure measures in the field of (Digital) Humanities and the orchestration of distributed software systems. Since March 2018, Philip has been part of the DFG-LIS project ""SustainLife"" at the Data Center for the Humanities (DCH).</p>
<p dir=""ltr""><strong></strong></p>
<p dir=""ltr""><strong>Acknowledgements</strong></p>
<p dir=""ltr"">This poster is partially funded by the DFG-LIS project “SustainLife” (GEPRIS 379522012).</p>
<p dir=""ltr""></p>
<p dir=""ltr""><strong>References</strong></p>
<ul><li dir=""ltr"">
<p dir=""ltr"">Barzen, J. and Blumtritt, J. and Breitenbücher, U. and Kronenwett, S. and Leymann, F. and Mathiak, B. and Neuefeind, C. (2018). SustainLife – Erhalt lebender, digitaler Systeme für die Geisteswissenschaften. In: Book of Abstracts of the 5th annual Conference of the Digital Humanities im deutschsprachigen Raum (DHd 2018), pp. 471-474.</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">Bingert, S. and Blumtritt, J. and Buddenbohm, S. and Engelhardt, C. and Kronenwett, S. and Kurzawe, D. (2016). Anwendungskonservierung und die Nachhaltigkeit von Forschungsanwendungen. In: Forschungsdaten​ ​in​ ​den​ ​Geisteswissenschaften​ ​(FORGE​ ​2016),​ pp. 14-16.</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">Breitenbücher, U. and Endres, C. and Képes, K. and Kopp, O. and Leymann, F. and Wagner, S. and Wettinger, J. and Zimmermann, M. (2016). The OpenTOSCA Ecosystem. Concept & Tools. In: European Space Project on Smart Systems, Big Data, Future Internet. Towards Serving the Grand Societal Challenges. Volume #1, pp. 112-130.</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">Kopp, O. and Binz, T. and Breitenbücher, U. and Leymann, F. (2013). Winery – A Modeling Tool for TOSCA-based Cloud Applications. In: Proceedings of the 11th International Conference on Service-Oriented Computing (ICSOC 2013), pp. 700-704.</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">Neuefeind, C. and Harzenetter, L. and Schildkamp, P. and Breitenbücher, U. and Mathiak, B. and Barzen, J. and Leymann, F. (2018). The SustainLife Project – Living Systems in Digital Humanities. In: Proceedings of the 12th Advanced Summer School on Service-Oriented Computing (SummerSoC 2018) (IBM Research Report RC25681), pp. 101-112.</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">Neuefeind, C. and Schildkamp, P. and Mathiak, B. and Marčić, A. and Hentschel, F. and Harzenetter, L. and Breitenbücher, U. and Barzen, J. and Leymann, F. (2019). Sustaining the Musical Competitions Database. A TOSCA-based Approach to Application Preservation in the Digital Humanities. In: Book of Abstracts of the 29th Digital Humanities Conference (DH 2019), https://dev.clariah.nl/files/dh2019/boa/0574.html (retrieved: 2019-09-10).</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">OASIS (2013). Topology and Orchestration Specification for Cloud Applications Version 1.0, http://docs.oasis-open.org/tosca/TOSCA/v1.0/TOSCA-v1.0.html (retrieved: 2019-09-10).</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">OASIS (2019). TOSCA Simple Profile in YAML Version 1.2, http://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.2/TOSCA-Simple-Profile-YAML-v1.2.html (retrieved: 2019-09-10).</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">Sahle, P. and Kronenwett, S. (2013). Jenseits der Daten. Überlegungen zu Datenzentren für die Geisteswissenschaften am Beispiel des Kölner Data Center for the Humanities. In: LIBREAS. Library Ideas #23, pp. 76-96.</p>
</li>
</ul>","Abstract
The project ""SustainLife – Sustaining Living Digital Systems in the Humanities"" that is currently running at the Institute of Architecture of Application Systems (IAAS, University of Stuttgart) and the Data Center for the Humanities (DCH, University of Cologne) deals with the conservation of research applications in the field of Digital Humanities (DH). By employing the TOSCA standard (Topology and Orchestration Specification for Cloud Applications) to fully automate the deployment of DH applications and to keep them available in the long term, we try to tackle the problem of software obsolescence in the field of DH. To interactively demonstrate our approach to the international DH community, we would like to give a workshop on the topic ""Modelling and Maintaining Research Applications in TOSCA"" in the run-up to the DH 2020 conference. Thereinwe will show how to model (DH) software systems with TOSCA and share experiences and best practices on how to work with the OpenTOSCA ecosystem, an open-source implementation of the TOSCA standard.
The Problem
The establishment of the DH as an independent scientific research area as well as the increasing usage of digital methods in the research process require adjustments to common result assurance practices. For example, the long-term archiving (LTA) of primary research data uses well-established practices such as employing standardized data formats and forwarding data to permanent repositories. However, the fact that digital artifacts generated in DH-oriented research do not only consist of primary data but also contain research software is mostly disregarded (Sahle and Kronenwett, 2013). Moreover, the variety of DH research outcomes includes so-called ""living systems"" in which the software to present, access or analyze the data represents an essential part of the actual research output (Bingert et al., 2016). In contrast to classical research results such as monographs or encyclopedias, living systems cannot be served long-term without maintenance as their instantiation, supervision, and permanent provisioning represent major technical, organizational, and financial challenges. Furthermore, the heterogeneity of the research software generated in the DH requires a highly flexible preservation strategy, i.e., a suitable technology that ensures standardization, reusability, and archiving of as many digital artifacts as possible (Barzen et al., 2018). In addition to the aforementioned challenges, i.e., heterogeneity, underfunding, and obsolescence of digital artifacts, scientific practice requires long-term interoperability and traceability of all research outcomes. With regard to digital systems, these requirements are (1) constant accessibility, (2) the possibility of error-free operation, and (3) the ability to reconstruct any stage of development of a research application at any time without major structural difficulties.
Our Approach
The TOSCA standard (OASIS, 2013 and 2019) allows software systems to be modelled, provisioned, and deployed in a standardized and provider-independent manner. Thus, it is suitable for long-term archiving and operation of research applications produced within the field of DH (Neuefeind et al., 2018 and 2019). Following the TOSCA standard, applications are modelled in “Topology Templates” by describing their components and their relations amongst each other: Components are represented as “Node Templates”, while relations are modeled as “Relationship Templates”. Moreover, the semantics of a Node Template or Relationship Template are dictated by reusable types, i.e., “Node Types” and “Relationship Types” respectively. For example, a Python web application can be modelled as a Node Template that is an instance of the ""Python Application"" Node Type. To express that the Python Application accesses a MySQL database, a second Node Template that is of type ""MySQL Database"" can be added to the Topology Template. Then, the connection between both components can be described by a Relationship Template that is an instance of the Relationship Type ""connectsTo"". Additionally, to specify that both components are running on an Ubuntu virtual machine (VM), a Node Template of type ""Ubuntu VM"" can be added, while Relationship Templates of type “hostedOn” between the Python Application Node Template and the VM Node Template, as well as between the MySQL Database and the VM describe their respective hosting relations.
Hereby, TOSCA's type system enables the modelling of reusable component types, e.g., the ""Python Application"" Node Type, which can be reused in multiple Topology Templates describing different applications. Therefore, synergic effects emerge as existing Node Types can be reused in other Topology Templates, easing the modelling of new applications. In addition, the open-source TOSCA implementation OpenTOSCA (Breitenbücher et al., 2016) offers the possibility to graphically model applications using the TOSCA editor “Winery” (Kopp et al., 2013) which further simplifies the creation of new applications by providing drag-and-drop modeling capabilities.
Workshop Curriculum
During our four hour workshop, we will (1) give an overview to different solutions for long-term preservation of living systems and (2) describe the modeling language TOSCA. Based on these theoretical units, practical tasks will introduce (3) the modelling of an existing application using TOSCA and (4) how applications can be deployed using the OpenTOSCA ecosystem. Thus, by combining the theoretical foundations and the practical application of TOSCA, the participants will be able to model (research) software systems according to the standard and provision and deploy applications using the OpenTOSCA ecosystem.
The practical tasks are structured as follows: (1) Identify the components of an application and (2) describe them and their relations among each other in an TOSCA-based application topology, i.e., in a Topology Template. By fragmenting an application into its components and mapping them to TOSCA Node Types, the Topology Template describing the application can then be modelled using the OpenTOSCA ecosystem. Afterwards (3), the modelled TOSCA application will be deployed by the OpenTOSCA runtime. Moreover, by sharing our experiences and best practices in using OpenTOSCA with the community, we will introduce concepts such as ""software stacks"" in a practical way.
Target Group
The workshop is primarily designed for data center employees, libraries and other institutions focusing on infrastructures for long-term archiving and operation of heterogeneous software systems. Previous experience in dealing with Linux and writing shell scripts as well as with software stacks and service orchestration are helpful but not necessary for a successful participation. To provide a productive context for communicating the described content and to enable individual consultation and support, we designed the workshop for about 20 participants but limit it to a maximum of 30 participants.
Technical Prerequisites
For a successful participation in the workshop, it is necessary that each participant brings his/her own laptop. Although a shared instance of the OpenTOSCA ecosystem will be provided, it is desirable that all participants set up an OpenTOSCA instance on their work equipment prior to the workshop in order to perform modelling and deployment tasks on their own devices. Therefore, registered participants will be provided with all necessary information about system requirements and how to setup OpenTOSCA prior to the workshop. Furthermore, relevant documentation, publications, and manuals will be provided both in advance and in the context of the workshop. In addition, a stable internet connection as well as a sufficient number of power outlets for all electronic devices are indispensable. 
About the Instructors
Uwe Breitenbücher is a research staff member and postdoc at the Institute of Architecture of Application Systems (IAAS) at the University of Stuttgart, Germany. His research vision is to improve cloud application provisioning and application management by automating the application of management patterns. Uwe was part of the CloudCycle project, in which the OpenTOSCA Ecosystem was developed. His current research interests include cyber-physical systems, blockchains, and microservices.
Anna Fischer is a research assistant at the Data Center for the Humanities (DCH) at the University of Cologne and joined the “SustainLife” Project in January 2020. Her recent research and working activities have focused on data management and software development for natural language processing tasks, e.g., in collaboration with one of the chairs for Romance linguistics at the University of Cologne. 
Lukas Harzenetter is a research associate at the Institute of Architecture of Application Systems (IAAS) at the University of Stuttgart, Germany. He received his Master of Science degree from the University of Stuttgart in Software Engineering in 2018. His research interests are in the field of cloud deployment and management models focusing on the development and change of such models over time. Lukas is part of the “SustainLife” project which is working on sustainable application deployments in the domain of digital humanities.
Frank Leymann is a full professor of computer science and director of the Institute of Architecture of Application Systems (IAAS) at the University of Stuttgart, Germany. His research interests include service-oriented architectures and associated middleware, workflow- and business process management, cloud computing and associated systems management aspects, and patterns. Frank is co-author of more than 400 peer-reviewed papers, about 70 patents, and several industry standards. He is an elected member of the Academy of Europe.
Brigitte Mathiak is chairwoman of the Data Center for the Humanities (DCH) and is particularly interested in data management and text mining. The idea for the ""SustainLife"" project arose after she had experienced again and again how living systems have to be abandoned or neglected. She is Junior Professor for Digital Humanities at the University of Cologne and Senior Scientist at the Leibniz Institute for the Social Sciences (GESIS).
Claes Neuefeind is a postdoc at the Cologne Center for eHumanities (CCeH) at the University of Cologne. He worked with Philip Schildkamp and Lukas Harzenetter on the DFG-LIS project ""SustainLife"" until October 2019 and changed for a position that is responsible for coordinating the Digital Humanities of the North Rhine-Westphalian Academy of Sciences and the Arts office.
Philip Schildkamp has been researching since 2015 and teaching since 2017 at the University of Cologne. He studied sociology, psychology, and Digital Humanities information processing. The main topics of his employment are technical infrastructure measures in the field of (Digital) Humanities and the orchestration of distributed software systems. Since March 2018, Philip has been part of the DFG-LIS project ""SustainLife"" at the Data Center for the Humanities (DCH).
Acknowledgements
This poster is partially funded by the DFG-LIS project “SustainLife” (GEPRIS 379522012).
References
Barzen, J. and Blumtritt, J. and Breitenbücher, U. and Kronenwett, S. and Leymann, F. and Mathiak, B. and Neuefeind, C. (2018). SustainLife – Erhalt lebender, digitaler Systeme für die Geisteswissenschaften. In: Book of Abstracts of the 5th annual Conference of the Digital Humanities im deutschsprachigen Raum (DHd 2018), pp. 471-474.
Bingert, S. and Blumtritt, J. and Buddenbohm, S. and Engelhardt, C. and Kronenwett, S. and Kurzawe, D. (2016). Anwendungskonservierung und die Nachhaltigkeit von Forschungsanwendungen. In: Forschungsdaten​ ​in​ ​den​ ​Geisteswissenschaften​ ​(FORGE​ ​2016),​ pp. 14-16.
Breitenbücher, U. and Endres, C. and Képes, K. and Kopp, O. and Leymann, F. and Wagner, S. and Wettinger, J. and Zimmermann, M. (2016). The OpenTOSCA Ecosystem. Concept & Tools. In: European Space Project on Smart Systems, Big Data, Future Internet. Towards Serving the Grand Societal Challenges. Volume #1, pp. 112-130.
Kopp, O. and Binz, T. and Breitenbücher, U. and Leymann, F. (2013). Winery – A Modeling Tool for TOSCA-based Cloud Applications. In: Proceedings of the 11th International Conference on Service-Oriented Computing (ICSOC 2013), pp. 700-704.
Neuefeind, C. and Harzenetter, L. and Schildkamp, P. and Breitenbücher, U. and Mathiak, B. and Barzen, J. and Leymann, F. (2018). The SustainLife Project – Living Systems in Digital Humanities. In: Proceedings of the 12th Advanced Summer School on Service-Oriented Computing (SummerSoC 2018) (IBM Research Report RC25681), pp. 101-112.
Neuefeind, C. and Schildkamp, P. and Mathiak, B. and Marčić, A. and Hentschel, F. and Harzenetter, L. and Breitenbücher, U. and Barzen, J. and Leymann, F. (2019). Sustaining the Musical Competitions Database. A TOSCA-based Approach to Application Preservation in the Digital Humanities. In: Book of Abstracts of the 29th Digital Humanities Conference (DH 2019), https://dev.clariah.nl/files/dh2019/boa/0574.html (retrieved: 2019-09-10).
OASIS (2013). Topology and Orchestration Specification for Cloud Applications Version 1.0, http://docs.oasis-open.org/tosca/TOSCA/v1.0/TOSCA-v1.0.html (retrieved: 2019-09-10).
OASIS (2019). TOSCA Simple Profile in YAML Version 1.2, http://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.2/TOSCA-Simple-Profile-YAML-v1.2.html (retrieved: 2019-09-10).
Sahle, P. and Kronenwett, S. (2013). Jenseits der Daten. Überlegungen zu Datenzentren für die Geisteswissenschaften am Beispiel des Kölner Data Center for the Humanities. In: LIBREAS. Library Ideas #23, pp. 76-96.","philip.schildkamp@uni-koeln.de, lukas.harzenetter@iaas.uni-stuttgart.de, frank.leymann@iaas.uni-stuttgart.de, bmathiak@uni-koeln.de, c.neuefeind@uni-koeln.de, uwe.breitenbuecher@iaas.uni-stuttgart.de",Workshop/Tutorial 4
121,"Chokshi, Crystal Nicole","University of Calgary, Canada",Gmail’s Smart Compose: A Critical Composit(ion),Gmail’s Smart Compose: A Critical Composit(ion),"algorithmically-mediated writing, Smart Compose, critical algorithm studies, critical media studies, linguistic capitalism","English, North America, Contemporary, information retrieval and querying algorithms and methods, text mining and analysis, Communication studies, Media studies","<p>While Google once merely monitored users’ words, today the company literally writes them. This is thanks to Smart Compose, a word-prediction algorithm that Google has launched in Gmail and Google Docs. The algorithm depends on Google’s meticulous recording and machine-reading of the personal data of an untold number of its 1.5 billion Gmail users, leveraging users’ words and writing for the development of the technology. In this context, language is particularly vulnerable to corporate intervention and manipulation. As such, this presentation carefully considers Fréderic Kaplan’s (2014) call to action: “through… the advent of algorithms as a new media, something is likely happen [<em>sic</em>] to language, and, although we are not yet sure what it will be, new tools must be built in order to understand this global linguistic evolution” (p. 62).</p>
<p>Responding to Kaplan's call, I report on experiments with Smart Compose in which I am manually transcribing more than 50,000 words from published texts and, subsequently, annotating and visualizing input to and output from the algorithm. These experiments are part of my larger doctoral project that seeks to locate the shifting ""semantic coordinates"" (Striphas, 2015, p. 398) of ""language,"" ""words,"" and ""writing"" in an algorithmic culture. Applying the framework of data colonialism (Couldry & Mejias, 2019), I argue that word-prediction algorithms such as Smart Compose must necessarily shift our understanding of these terms when words become data (Thornton, 2019) and writing becomes a datafied practice.</p>
<p>More broadly, I suggest that in place of the question posed by Siva Vaidhyanathan (2011) some years ago—“what do we gain and what do we lose by inviting Google to be the lens through which we see the world?” (p. 9)—we must, urgently and necessarily, ask this: what do we gain and what do we lose by allowing Google to offer the words through which we write the world?</p>
<p align=""center""><strong>References</strong></p>
<p>Couldry, N. & Mejias, U.A. (2019). Data colonialism: Rethinking big data’s relation to the contemporary subject. Television & New Media, 20(4), 336-349. https://doi.org/10.1177/1527476418796632</p>
<p>Kaplan, F. (2014). Linguistic capitalism and algorithmic mediation. <em>Representations,</em> 127(1), 57-63. https://doi.org/10.1525/rep.2014.127.1.57</p>
<p>Striphas, T. (2015). Algorithmic culture. European Journal of Cultural Studies, 18(4-5), 395-412. https://doi.org/10.1177/1367549415577392</p>
<p>Thornton, P. (2019). Language in the age of algorithmic reproduction: A critique of linguistic capitalism [Unpublished doctoral dissertation]. Royal Holloway, University of London.  </p>
<p>Vaidhyanathan, S. (2011). The googlization of everything (and why we should worry). University of California Press.</p>
","While Google once merely monitored users’ words, today the company literally writes them. This is thanks to Smart Compose, a word-prediction algorithm that Google has launched in Gmail and Google Docs. The algorithm depends on Google’s meticulous recording and machine-reading of the personal data of an untold number of its 1.5 billion Gmail users, leveraging users’ words and writing for the development of the technology. In this context, language is particularly vulnerable to corporate intervention and manipulation. As such, this presentation carefully considers Fréderic Kaplan’s (2014) call to action: “through… the advent of algorithms as a new media, something is likely happen [sic] to language, and, although we are not yet sure what it will be, new tools must be built in order to understand this global linguistic evolution” (p. 62).
Responding to Kaplan's call, I report on experiments with Smart Compose in which I am manually transcribing more than 50,000 words from published texts and, subsequently, annotating and visualizing input to and output from the algorithm. These experiments are part of my larger doctoral project that seeks to locate the shifting ""semantic coordinates"" (Striphas, 2015, p. 398) of ""language,"" ""words,"" and ""writing"" in an algorithmic culture. Applying the framework of data colonialism (Couldry & Mejias, 2019), I argue that word-prediction algorithms such as Smart Compose must necessarily shift our understanding of these terms when words become data (Thornton, 2019) and writing becomes a datafied practice.
More broadly, I suggest that in place of the question posed by Siva Vaidhyanathan (2011) some years ago—“what do we gain and what do we lose by inviting Google to be the lens through which we see the world?” (p. 9)—we must, urgently and necessarily, ask this: what do we gain and what do we lose by allowing Google to offer the words through which we write the world?
References
Couldry, N. & Mejias, U.A. (2019). Data colonialism: Rethinking big data’s relation to the contemporary subject. Television & New Media, 20(4), 336-349. https://doi.org/10.1177/1527476418796632
Kaplan, F. (2014). Linguistic capitalism and algorithmic mediation. Representations, 127(1), 57-63. https://doi.org/10.1525/rep.2014.127.1.57
Striphas, T. (2015). Algorithmic culture. European Journal of Cultural Studies, 18(4-5), 395-412. https://doi.org/10.1177/1367549415577392
Thornton, P. (2019). Language in the age of algorithmic reproduction: A critique of linguistic capitalism [Unpublished doctoral dissertation]. Royal Holloway, University of London.  
Vaidhyanathan, S. (2011). The googlization of everything (and why we should worry). University of California Press.",crystal.chokshi1@ucalgary.ca,Short Presentation
123,"Horak, Laura","Carleton University, Canada",Transing DH: Adopting a Trans-Centric Approach to Building the Transgender Media Portal,Transing DH: Adopting a Trans-Centric Approach to Building the Transgender Media Portal,"transgender, cinema, database, ethics, feminist","English, North America, 20th Century, Contemporary, database creation, management, and analysis, public humanities collaborations and methods, Film and cinema arts studies, Transgender and non-binary studies","<p>Guided by the fields of intersectional feminist digital humanities and transgender studies, this talk explores our team’s efforts to adopt a trans-centric approach to building a collaborative online tool to investigate and publicize films made by trans, Two Spirit, nonbinary, intersex, and gender-nonconforming people.</p>
","Guided by the fields of intersectional feminist digital humanities and transgender studies, this talk explores our team’s efforts to adopt a trans-centric approach to building a collaborative online tool to investigate and publicize films made by trans, Two Spirit, nonbinary, intersex, and gender-nonconforming people.",laura.horak@carleton.ca,Short Presentation
124,"Satlow, Michael ; Sperling, Michael "," Brown University, United States of America;  New York University, United States of America",The Rabbinic Social Network,The Rabbinic Social Network,"historical networks, gephi, pattern recognition","Asia, English, BCE-4th Century, 5th-14th Century, network analysis and graphs theory and application, History, Theology and religious studies","<p align=""center""></p>
<p>This project attempts to apply the techniques of social network analysis (SNA) and visualization to the representations of rabbinic interactions in the Babylonian Talmud, a sprawling text written in Hebrew and Aramaic and probably redacted in Babylonia (modern day Iraq) in the sixth century CE. Our goals are (1) to develop a workflow and methodology allowing us to visualize and analyze the interactions between rabbis as represented in the Babylonian Talmud; (2) to see if we could learn something new about the relationship between rabbis as represented in the Talmud and/or the process of its redaction; and (3) to present a public-facing interface allowing scholars to interact directly with our visualization.</p>
<p>Many of the research questions that drive this project go back more than a century. Pioneering work in Jewish studies (especially Albeck (1969); Margolioth (1987)) has attempted to detail the relationships between some of these rabbis. This work remains valuable, although it sometimes uses outdated methodological assumptions. Some of the relationships, for example, are reconstructed on the basis of stories about rabbis that most scholars today would understand as late, fictional creations. So too, scholars have long tried to understand the process by which the Babylonian Talmud was redacted (for summary of the scholarship on this, see Rubenstein 2013). Historians have also tried to understand the rabbis as a “network”, although without applying the quantitative tools now available (Hezser (1997); Lapin (2012)).</p>
<p>In this part of the project, we focused our attention on citation chains. Rabbis frequently say things in the name of other rabbis (e.g., “Rabbi X said in the name of Rabbi Y who said in the name of Rabbi Z” – these chains usually consist of two or three names but can go up to nine!). By focusing on simply the names in these chains (and not the content of what they reported), our work intersects with that of Zhitomirsky-Geffet and Prebor (2018) and Josh Waxman (2019). At the same time, both our workflow and the kinds of questions that we were asking of the network as a whole make it distinct.</p>
<p>The first step in our workflow was to identify each instance of a citation chain in the Hebrew/Aramaic text. In order to do this, we compiled a list of the names of all rabbis mentioned in ancient rabbinic literature (along with any aliases that they had) and assigned each a unique numeric identifier. The list was created through both automated and manual processes. Then, we created and ran a pattern matching program on a digital version of the “standard” printed edition (Vilna) of the Babylonian Talmud text to identify instances of rabbinic names and citation chains. Once identified, the program split the citations into “source” and “target” rabbis so we could identify who was citing whom. The results of the automated process were highly accurate as we verified through manual review of a statistically significant sample.</p>
<p>The program identified 5,245 citation instances. When grouped into unique interactions (e.g., Rabbi X may cite Rabbi Y twenty times, but we counted that as one unique interaction), we were left with 630 rabbis (our nodes) and 1217 unique interactions (our edges). We loaded our node and edges files into Gephi (Gephi) and UCINET. A visualization can be seen in Figure 1, which (using a Force Atlas 2 layout) groups the more connected rabbis toward the center.</p>
<p>Figure 1: Graph of All Rabbis in Babylonian Talmud Who Appear in Citation Chains</p>
<p>We have two major research findings. First, and less surprisingly, when separated into Modularity Classes through an unsupervised algorithm, the rabbis relatively cleanly separated into groups that clustered around rabbinic figures who themselves had many connections, which looks like a “school” structure (see Figure 2). Previous research has led us to expect this. Second, and more surprisingly, the rabbis at the centers of each of those circles were themselves densely and directly connected to each other. These ten or fifteen rabbis, over four centuries and two geographical locales, served as the backbone for the rabbinic network. It is still unclear to us whether these connections represent real social interactions or can better be explained as the result of later editing and redactional decisions and conventions.</p>
<p>Figure 2: Rabbis in Citation Chains in Babylonian Talmud in Modularity Classes</p>
<p>There are problems inherent in this data. The rabbis are themselves sometimes unsure about an attribution (and explicitly argue about it). Since rabbis sometimes shared names or nicknames, it is sometimes impossible to match with certainty a name with a distinct individual; in such cases we assigned the shared name to the more prominent rabbi. Moreover, we are just using one, easily available text. Manuscripts sometimes record these attributions differently. We feel that given the macro approach we took to this network, these problems become less significant. Nevertheless, they need to be better taken into account in future, more fine-grained analyses.</p>
<p>By the time this is published, we should have our data and code freely available on Github. It may take us longer to develop a public-facing interface, perhaps along the lines of “The Six Degrees of Francis Bacon.” We will also extend our approach to other interactions in the Babylonian Talmud (e.g., when a rabbi asks a question of another rabbi); to other rabbinic texts from this period; and to other manuscript versions of these texts.</p>
<p><strong>Bibliography</strong></p>
<p>Albeck, Ch. 1969. <em>Introduction to the Talmud Babli and Yerushalmi</em>. Tel Aviv: Dvir (in Hebrew).</p>
<p>Gephi: https://gephi.org/</p>
<p>Hezser, C. 1997. <em>The Social Structure of the Rabbinic Movement in Roman Palestine</em>. Tübingen: Mohr Siebeck.</p>
<p>Lapin, H. 2012. <em>Rabbis as Romans: The Rabbinic Movement in Palestine, 100-400 CE</em>. New York: Oxford University Press.</p>
<p>Margolioth, M. 1987 (rpt). <em>Encyclopedia of Talmudic and Geonic Literature</em>. Tel-Aviv: Y. Orenstein (in Hebrew).</p>
<p>Rubenstein, J. 2013. “Translator’s Introduction,” in David Weiss Halivni, <em>The Formation of the Babylonian Talmud</em> (New York: Oxford University Press), xvii-xxx.</p>
<p>Six Degrees of Francis Bacon: http://www.sixdegreesoffrancisbacon.com/?ids=10000473&min_confidence=60&type=network</p>
<p>UCINET: https://sites.google.com/site/ucinetsoftware/home</p>
<p>Waxman, J. “Mi vaMi: A Graph Databse of the Babylonian Talmud.” http://www.mivami.org/</p>
<p>Waxman, J. 2019. “A Graph Database of Scholastic Relationships in the Babylonian Talmud”: https://www.narcis.nl/dataset/RecordID/hdl%3A10411%2FK8HHQZ</p>
<p>Zhitomirsky-Geffet, M. and Prebor, G. 2019. “SageBook: Toward a Cross-Generational Social Network for the Jewish Sages’ Prosopography.” <em>Digital Scholarship in the Humanities</em> 34.3: 676-695 (https://doi.org/10.1093/llc/fqy065)</p>
","This project attempts to apply the techniques of social network analysis (SNA) and visualization to the representations of rabbinic interactions in the Babylonian Talmud, a sprawling text written in Hebrew and Aramaic and probably redacted in Babylonia (modern day Iraq) in the sixth century CE. Our goals are (1) to develop a workflow and methodology allowing us to visualize and analyze the interactions between rabbis as represented in the Babylonian Talmud; (2) to see if we could learn something new about the relationship between rabbis as represented in the Talmud and/or the process of its redaction; and (3) to present a public-facing interface allowing scholars to interact directly with our visualization.
Many of the research questions that drive this project go back more than a century. Pioneering work in Jewish studies (especially Albeck (1969); Margolioth (1987)) has attempted to detail the relationships between some of these rabbis. This work remains valuable, although it sometimes uses outdated methodological assumptions. Some of the relationships, for example, are reconstructed on the basis of stories about rabbis that most scholars today would understand as late, fictional creations. So too, scholars have long tried to understand the process by which the Babylonian Talmud was redacted (for summary of the scholarship on this, see Rubenstein 2013). Historians have also tried to understand the rabbis as a “network”, although without applying the quantitative tools now available (Hezser (1997); Lapin (2012)).
In this part of the project, we focused our attention on citation chains. Rabbis frequently say things in the name of other rabbis (e.g., “Rabbi X said in the name of Rabbi Y who said in the name of Rabbi Z” – these chains usually consist of two or three names but can go up to nine!). By focusing on simply the names in these chains (and not the content of what they reported), our work intersects with that of Zhitomirsky-Geffet and Prebor (2018) and Josh Waxman (2019). At the same time, both our workflow and the kinds of questions that we were asking of the network as a whole make it distinct.
The first step in our workflow was to identify each instance of a citation chain in the Hebrew/Aramaic text. In order to do this, we compiled a list of the names of all rabbis mentioned in ancient rabbinic literature (along with any aliases that they had) and assigned each a unique numeric identifier. The list was created through both automated and manual processes. Then, we created and ran a pattern matching program on a digital version of the “standard” printed edition (Vilna) of the Babylonian Talmud text to identify instances of rabbinic names and citation chains. Once identified, the program split the citations into “source” and “target” rabbis so we could identify who was citing whom. The results of the automated process were highly accurate as we verified through manual review of a statistically significant sample.
The program identified 5,245 citation instances. When grouped into unique interactions (e.g., Rabbi X may cite Rabbi Y twenty times, but we counted that as one unique interaction), we were left with 630 rabbis (our nodes) and 1217 unique interactions (our edges). We loaded our node and edges files into Gephi (Gephi) and UCINET. A visualization can be seen in Figure 1, which (using a Force Atlas 2 layout) groups the more connected rabbis toward the center.
Figure 1: Graph of All Rabbis in Babylonian Talmud Who Appear in Citation Chains
We have two major research findings. First, and less surprisingly, when separated into Modularity Classes through an unsupervised algorithm, the rabbis relatively cleanly separated into groups that clustered around rabbinic figures who themselves had many connections, which looks like a “school” structure (see Figure 2). Previous research has led us to expect this. Second, and more surprisingly, the rabbis at the centers of each of those circles were themselves densely and directly connected to each other. These ten or fifteen rabbis, over four centuries and two geographical locales, served as the backbone for the rabbinic network. It is still unclear to us whether these connections represent real social interactions or can better be explained as the result of later editing and redactional decisions and conventions.
Figure 2: Rabbis in Citation Chains in Babylonian Talmud in Modularity Classes
There are problems inherent in this data. The rabbis are themselves sometimes unsure about an attribution (and explicitly argue about it). Since rabbis sometimes shared names or nicknames, it is sometimes impossible to match with certainty a name with a distinct individual; in such cases we assigned the shared name to the more prominent rabbi. Moreover, we are just using one, easily available text. Manuscripts sometimes record these attributions differently. We feel that given the macro approach we took to this network, these problems become less significant. Nevertheless, they need to be better taken into account in future, more fine-grained analyses.
By the time this is published, we should have our data and code freely available on Github. It may take us longer to develop a public-facing interface, perhaps along the lines of “The Six Degrees of Francis Bacon.” We will also extend our approach to other interactions in the Babylonian Talmud (e.g., when a rabbi asks a question of another rabbi); to other rabbinic texts from this period; and to other manuscript versions of these texts.
Bibliography
Albeck, Ch. 1969. Introduction to the Talmud Babli and Yerushalmi. Tel Aviv: Dvir (in Hebrew).
Gephi: https://gephi.org/
Hezser, C. 1997. The Social Structure of the Rabbinic Movement in Roman Palestine. Tübingen: Mohr Siebeck.
Lapin, H. 2012. Rabbis as Romans: The Rabbinic Movement in Palestine, 100-400 CE. New York: Oxford University Press.
Margolioth, M. 1987 (rpt). Encyclopedia of Talmudic and Geonic Literature. Tel-Aviv: Y. Orenstein (in Hebrew).
Rubenstein, J. 2013. “Translator’s Introduction,” in David Weiss Halivni, The Formation of the Babylonian Talmud (New York: Oxford University Press), xvii-xxx.
Six Degrees of Francis Bacon: http://www.sixdegreesoffrancisbacon.com/?ids=10000473&min_confidence=60&type=network
UCINET: https://sites.google.com/site/ucinetsoftware/home
Waxman, J. “Mi vaMi: A Graph Databse of the Babylonian Talmud.” http://www.mivami.org/
Waxman, J. 2019. “A Graph Database of Scholastic Relationships in the Babylonian Talmud”: https://www.narcis.nl/dataset/RecordID/hdl%3A10411%2FK8HHQZ
Zhitomirsky-Geffet, M. and Prebor, G. 2019. “SageBook: Toward a Cross-Generational Social Network for the Jewish Sages’ Prosopography.” Digital Scholarship in the Humanities 34.3: 676-695 (https://doi.org/10.1093/llc/fqy065)","michael_satlow@brown.edu, mike.sperling@rocketmail.com",Lightning
125,"Bowker, Lynne","University of Ottawa, Canada",Improving machine translation literacy to facilitate and enhance scholarly communication,Improving machine translation literacy to facilitate and enhance scholarly communication,digital literacy; machine translation; machine translation literacy; human-computer-interaction; scholarly communication,"English, North America, Contemporary, artificial intelligence and machine learning, curricular and pedagogical development and analysis, Language acquisition, Translation studies","<p>English is the main language of scholarly communication, but, most researchers are not native English speakers. Contemporary machine translation approaches such as neural machine translation (NMT) are data-driven and use artificial-intelligence-based machine learning techniques; however, such tools rarely produce high quality output of specialized text without human intervention. There is an emerging need for machine translation (MT) literacy among non-Anglpohone students and faculty who must both read and write in English in order to participate fully in the scholarly communication process. We designed and pilot tested a machine translation literacy workshop to help researchers use MT more effectively for scholarly tasks such as: 1) search and discovery of scholarly texts; 2) reading and evaluating scholarly texts; 3) research communication in international teams; and 4) writing for scholarly publishing. Pre- and post-workshop surveys were used to evaluate the success of the workshop and recommend improvements for future iterations.</p>
","English is the main language of scholarly communication, but, most researchers are not native English speakers. Contemporary machine translation approaches such as neural machine translation (NMT) are data-driven and use artificial-intelligence-based machine learning techniques; however, such tools rarely produce high quality output of specialized text without human intervention. There is an emerging need for machine translation (MT) literacy among non-Anglpohone students and faculty who must both read and write in English in order to participate fully in the scholarly communication process. We designed and pilot tested a machine translation literacy workshop to help researchers use MT more effectively for scholarly tasks such as: 1) search and discovery of scholarly texts; 2) reading and evaluating scholarly texts; 3) research communication in international teams; and 4) writing for scholarly publishing. Pre- and post-workshop surveys were used to evaluate the success of the workshop and recommend improvements for future iterations.",lbowker@uottawa.ca,Poster
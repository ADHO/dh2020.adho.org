<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>From Millions of Words to a Single Phrase: Examination of the TXM Software in Hebrew</title><meta name="author" content="Lengo, Maxim"><meta name="generator" content="Text Encoding Initiative Consortium XSLT stylesheets"><meta name="DC.Title" content="From Millions of Words to a Single Phrase: Examination of the TXM Software in Hebrew"><meta name="DC.Type" content="Text"><meta name="DC.Format" content="text/html"><link href="http://www.tei-c.org/release/xml/tei/stylesheet/tei.css" rel="stylesheet" type="text/css"><link rel="stylesheet" media="print" type="text/css" href="http://www.tei-c.org/release/xml/tei/stylesheet/tei-print.css"></head><body class="simple" id="TOP"><div class="stdheader autogenerated"><h1 class="maintitle"><span class="titlem">From Millions of Words to a Single Phrase</span><span class="titles">Examination of the TXM Software in Hebrew</span></h1></div><div class="dhconvalidator-xml-link"><a href="200_FromMillionsofWordstoaSinglePhraseExaminationoftheTXMSoftwareinHebrew.xml">XML</a></div><div class="DH-Heading1" id="index.xml-body.1_div.1"><h2 class="DH-Heading1"><span class="headingNumber">1. </span><span class="head">Abstract</span></h2><p>Following the technological development during the second half of the 20<sup>th</sup> Century, linguistic researchers have begun for the first time to analyze corpora of big data. One research methodology, which was developed for lexicometry and text statistical analysis, is Textometry. I.e., the attempt to combine various statistical analysis techniques, such as factorial correspondence analysis (Benz�cri, 1977) and hierarchical ascendant classification (Ward Jr, 1963), with full-text search techniques such as kwic concordances (Luhn, 1960), in order to trace the precise original editorial context of any textual event participating to the analysis.</p><p>The TXM software (Heiden, 2010), which was developed in France as a modular platform of a new generation of textometrical research and which I adapted to Hebrew, gives the ability to analyze a large corpus of texts as in my research, by using tools and methods based on linguistics and discourse analysis, that is, decomposing the text into factors and elements, carrying out statistical analysis, identifying the hidden social patterns, and then restoring the corpus to its original mode.</p><p>As a Ph.D. student in the discipline of Social Sciences, my research focuses on the image repair theory (Benoit, 2015) as an ensemble of strategies such as evading responsibility and reducing offensiveness, used by individuals, organizations and groups in order to repair their image during times of crisis. It examines the ways in which rhetorical measures are used in online verbal exchanges among users of the online social networks who attempt to repair their personal image. To achieve my goal, I use the TXM software to analyze a corpus of more than eight million words in 365 Facebook posts, which were published by the Israeli Prime Minister Benjamin Netanyahu during his current affairs, and more than 285,000 comments, made by the users. Netanyahu's Affairs are four police investigations in which he is involved as a suspect or has given a testimony.</p><p>During the poster session, I will present a review of some tools I used with the TXM software during the digitized analysis process in my Ph.D. research and the way they allowed me to recognize the following key phrase used by Netanyahu: "They have the media, we have you". Among these tools are Progression, which is the frequency of occurrence of one term throughout the text corpus; Co-occurrence, which is the frequency of occurrence of two terms in a text corpus alongside each other in a certain order; and Specificity, which is the score a term is given based on its occurrence in the corresponding part of the corpus relative to the one in the entire corpus, and indicates whether it is overused, underused or useless.</p><p align="center"></p><p align="center"><strong>Bibliography</strong></p><p align="center"></p><p>Benoit, W. L. (2015). <em>Accounts, excuses, and apologies: Image repair theory and </em><em>research</em>. Albany, New York: State University of New York Press.</p><p>Benz�cri, J. P. (1977). Sur l'analyse des tableaux binaires associ�s � une correspondance multiple. <em>Cahiers de l'Analyse des Donn</em><em>�</em><em>es, 2(1)</em>, 55-71.</p><p>Heiden, S. (2010). The TXM Platform: Building Open-Source Textual Analysis Software Compatible with the TEI Encoding Scheme. In R. Otoguro, Ishikawa, K., Umemoto, H., Yoshimoto, K., Harada, Y. (Eds.), <em>24th Pacific Asia Conference on Language, Information and Computation - PACLIC24</em> (Pp. 389-398). Waseda University, Sendai, Japan: Institute for Digital Enhancement of Cognitive Development.</p><p>Luhn, H. P. (1960). Key word?in?context index for technical literature (kwic index). <em>American Documentation, 11(4)</em>, 288-295.</p><p>Ward Jr, J. H. (1963). Hierarchical grouping to optimize an objective function. <em>Journal </em><em>of the American statistical association, 58(301)</em>, 236-244.</p></div><div class="stdfooter autogenerated"><address>Maxim Lengo (maximlengo@gmail.com), Bar-Ilan University, Israel</address></div></body></html>